{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2806e93ae3354dc490ebadfbff379104": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d36d92d1ab1945fd9c00356fd0d54786",
              "IPY_MODEL_82c68382e77f4effb557c1722178ddcf",
              "IPY_MODEL_438b9f30c0cf4dac8fb7e66045d23fc8"
            ],
            "layout": "IPY_MODEL_ad018fb2626e425983134e268bc27f69"
          }
        },
        "d36d92d1ab1945fd9c00356fd0d54786": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d982bdc05e64295822a3a09a425adb5",
            "placeholder": "​",
            "style": "IPY_MODEL_e6c8c322ae234709a1908d83d9467274",
            "value": "Loading pipeline components...: 100%"
          }
        },
        "82c68382e77f4effb557c1722178ddcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a12b20f921e4968a61841df5fde18d6",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7ac2aa1b9931492787b9f1f3a28ec588",
            "value": 7
          }
        },
        "438b9f30c0cf4dac8fb7e66045d23fc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7344b201edfa4371950b673b05766aca",
            "placeholder": "​",
            "style": "IPY_MODEL_84cc02651d72429eb4d35b6b3353fbc0",
            "value": " 7/7 [00:10&lt;00:00,  1.58s/it]"
          }
        },
        "ad018fb2626e425983134e268bc27f69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d982bdc05e64295822a3a09a425adb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6c8c322ae234709a1908d83d9467274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a12b20f921e4968a61841df5fde18d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ac2aa1b9931492787b9f1f3a28ec588": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7344b201edfa4371950b673b05766aca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84cc02651d72429eb4d35b6b3353fbc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4356d8acd70e4a94beb99a121665accf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a612db3f81143ce99721f3c3b6b6eb4",
              "IPY_MODEL_7d3e03808a574d1cb1fe93a761bd4c0a",
              "IPY_MODEL_a33a5c6783e94c2d92acb04abd89c280"
            ],
            "layout": "IPY_MODEL_6a55a765bf544ceb89573dbee24034b2"
          }
        },
        "6a612db3f81143ce99721f3c3b6b6eb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f34f1dcd4804d058a7184a075c1ec9d",
            "placeholder": "​",
            "style": "IPY_MODEL_ad5465148fda4cb88018bf07cf5a5e0d",
            "value": "100%"
          }
        },
        "7d3e03808a574d1cb1fe93a761bd4c0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6f03520eb644554bae8c8038c3eb59a",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c976f05348ff424790af85fc3e53e916",
            "value": 50
          }
        },
        "a33a5c6783e94c2d92acb04abd89c280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_041582b2fad44115aa6aac1a234581d2",
            "placeholder": "​",
            "style": "IPY_MODEL_bba24bd6295f4b4f990dadda659d50ab",
            "value": " 50/50 [00:08&lt;00:00,  6.44it/s]"
          }
        },
        "6a55a765bf544ceb89573dbee24034b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f34f1dcd4804d058a7184a075c1ec9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad5465148fda4cb88018bf07cf5a5e0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6f03520eb644554bae8c8038c3eb59a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c976f05348ff424790af85fc3e53e916": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "041582b2fad44115aa6aac1a234581d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bba24bd6295f4b4f990dadda659d50ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers transformers accelerate safetensors torch\n"
      ],
      "metadata": {
        "id": "vxp6CGu16nTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b253ef4-9761-433c-b902-a35d830ae7fe",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.33.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.5.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.31.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers) (11.2.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.27.0->diffusers) (1.1.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2025.4.26)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m115.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "help(StableDiffusionPipeline)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VVUpwQC_pso",
        "outputId": "91426151-5e5c-41f5-c844-44ac2d06375a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class StableDiffusionPipeline in module diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion:\n",
            "\n",
            "class StableDiffusionPipeline(diffusers.pipelines.pipeline_utils.DiffusionPipeline, diffusers.pipelines.pipeline_utils.StableDiffusionMixin, diffusers.loaders.textual_inversion.TextualInversionLoaderMixin, diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin, diffusers.loaders.ip_adapter.IPAdapterMixin, diffusers.loaders.single_file.FromSingleFileMixin)\n",
            " |  StableDiffusionPipeline(vae: diffusers.models.autoencoders.autoencoder_kl.AutoencoderKL, text_encoder: transformers.models.clip.modeling_clip.CLIPTextModel, tokenizer: transformers.models.clip.tokenization_clip.CLIPTokenizer, unet: diffusers.models.unets.unet_2d_condition.UNet2DConditionModel, scheduler: diffusers.schedulers.scheduling_utils.KarrasDiffusionSchedulers, safety_checker: diffusers.pipelines.stable_diffusion.safety_checker.StableDiffusionSafetyChecker, feature_extractor: transformers.models.clip.image_processing_clip.CLIPImageProcessor, image_encoder: transformers.models.clip.modeling_clip.CLIPVisionModelWithProjection = None, requires_safety_checker: bool = True)\n",
            " |  \n",
            " |  Pipeline for text-to-image generation using Stable Diffusion.\n",
            " |  \n",
            " |  This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods\n",
            " |  implemented for all pipelines (downloading, saving, running on a particular device, etc.).\n",
            " |  \n",
            " |  The pipeline also inherits the following loading methods:\n",
            " |      - [`~loaders.TextualInversionLoaderMixin.load_textual_inversion`] for loading textual inversion embeddings\n",
            " |      - [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_weights`] for loading LoRA weights\n",
            " |      - [`~loaders.StableDiffusionLoraLoaderMixin.save_lora_weights`] for saving LoRA weights\n",
            " |      - [`~loaders.FromSingleFileMixin.from_single_file`] for loading `.ckpt` files\n",
            " |      - [`~loaders.IPAdapterMixin.load_ip_adapter`] for loading IP Adapters\n",
            " |  \n",
            " |  Args:\n",
            " |      vae ([`AutoencoderKL`]):\n",
            " |          Variational Auto-Encoder (VAE) model to encode and decode images to and from latent representations.\n",
            " |      text_encoder ([`~transformers.CLIPTextModel`]):\n",
            " |          Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).\n",
            " |      tokenizer ([`~transformers.CLIPTokenizer`]):\n",
            " |          A `CLIPTokenizer` to tokenize text.\n",
            " |      unet ([`UNet2DConditionModel`]):\n",
            " |          A `UNet2DConditionModel` to denoise the encoded image latents.\n",
            " |      scheduler ([`SchedulerMixin`]):\n",
            " |          A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n",
            " |          [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n",
            " |      safety_checker ([`StableDiffusionSafetyChecker`]):\n",
            " |          Classification module that estimates whether generated images could be considered offensive or harmful.\n",
            " |          Please refer to the [model card](https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5) for\n",
            " |          more details about a model's potential harms.\n",
            " |      feature_extractor ([`~transformers.CLIPImageProcessor`]):\n",
            " |          A `CLIPImageProcessor` to extract features from generated images; used as inputs to the `safety_checker`.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      StableDiffusionPipeline\n",
            " |      diffusers.pipelines.pipeline_utils.DiffusionPipeline\n",
            " |      diffusers.configuration_utils.ConfigMixin\n",
            " |      diffusers.utils.hub_utils.PushToHubMixin\n",
            " |      diffusers.pipelines.pipeline_utils.StableDiffusionMixin\n",
            " |      diffusers.loaders.textual_inversion.TextualInversionLoaderMixin\n",
            " |      diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin\n",
            " |      diffusers.loaders.lora_base.LoraBaseMixin\n",
            " |      diffusers.loaders.ip_adapter.IPAdapterMixin\n",
            " |      diffusers.loaders.single_file.FromSingleFileMixin\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __call__(self, prompt: Union[str, List[str]] = None, height: Optional[int] = None, width: Optional[int] = None, num_inference_steps: int = 50, timesteps: List[int] = None, sigmas: List[float] = None, guidance_scale: float = 7.5, negative_prompt: Union[str, List[str], NoneType] = None, num_images_per_prompt: Optional[int] = 1, eta: float = 0.0, generator: Union[torch._C.Generator, List[torch._C.Generator], NoneType] = None, latents: Optional[torch.Tensor] = None, prompt_embeds: Optional[torch.Tensor] = None, negative_prompt_embeds: Optional[torch.Tensor] = None, ip_adapter_image: Union[PIL.Image.Image, numpy.ndarray, torch.Tensor, List[PIL.Image.Image], List[numpy.ndarray], List[torch.Tensor], NoneType] = None, ip_adapter_image_embeds: Optional[List[torch.Tensor]] = None, output_type: Optional[str] = 'pil', return_dict: bool = True, cross_attention_kwargs: Optional[Dict[str, Any]] = None, guidance_rescale: float = 0.0, clip_skip: Optional[int] = None, callback_on_step_end: Union[Callable[[int, int, Dict], NoneType], diffusers.callbacks.PipelineCallback, diffusers.callbacks.MultiPipelineCallbacks, NoneType] = None, callback_on_step_end_tensor_inputs: List[str] = ['latents'], **kwargs)\n",
            " |          The call function to the pipeline for generation.\n",
            " |      \n",
            " |          Args:\n",
            " |              prompt (`str` or `List[str]`, *optional*):\n",
            " |                  The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`.\n",
            " |              height (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`):\n",
            " |                  The height in pixels of the generated image.\n",
            " |              width (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`):\n",
            " |                  The width in pixels of the generated image.\n",
            " |              num_inference_steps (`int`, *optional*, defaults to 50):\n",
            " |                  The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n",
            " |                  expense of slower inference.\n",
            " |              timesteps (`List[int]`, *optional*):\n",
            " |                  Custom timesteps to use for the denoising process with schedulers which support a `timesteps` argument\n",
            " |                  in their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is\n",
            " |                  passed will be used. Must be in descending order.\n",
            " |              sigmas (`List[float]`, *optional*):\n",
            " |                  Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in\n",
            " |                  their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed\n",
            " |                  will be used.\n",
            " |              guidance_scale (`float`, *optional*, defaults to 7.5):\n",
            " |                  A higher guidance scale value encourages the model to generate images closely linked to the text\n",
            " |                  `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.\n",
            " |              negative_prompt (`str` or `List[str]`, *optional*):\n",
            " |                  The prompt or prompts to guide what to not include in image generation. If not defined, you need to\n",
            " |                  pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale < 1`).\n",
            " |              num_images_per_prompt (`int`, *optional*, defaults to 1):\n",
            " |                  The number of images to generate per prompt.\n",
            " |              eta (`float`, *optional*, defaults to 0.0):\n",
            " |                  Corresponds to parameter eta (η) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies\n",
            " |                  to the [`~schedulers.DDIMScheduler`], and is ignored in other schedulers.\n",
            " |              generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n",
            " |                  A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make\n",
            " |                  generation deterministic.\n",
            " |              latents (`torch.Tensor`, *optional*):\n",
            " |                  Pre-generated noisy latents sampled from a Gaussian distribution, to be used as inputs for image\n",
            " |                  generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n",
            " |                  tensor is generated by sampling using the supplied random `generator`.\n",
            " |              prompt_embeds (`torch.Tensor`, *optional*):\n",
            " |                  Pre-generated text embeddings. Can be used to easily tweak text inputs (prompt weighting). If not\n",
            " |                  provided, text embeddings are generated from the `prompt` input argument.\n",
            " |              negative_prompt_embeds (`torch.Tensor`, *optional*):\n",
            " |                  Pre-generated negative text embeddings. Can be used to easily tweak text inputs (prompt weighting). If\n",
            " |                  not provided, `negative_prompt_embeds` are generated from the `negative_prompt` input argument.\n",
            " |              ip_adapter_image: (`PipelineImageInput`, *optional*): Optional image input to work with IP Adapters.\n",
            " |              ip_adapter_image_embeds (`List[torch.Tensor]`, *optional*):\n",
            " |                  Pre-generated image embeddings for IP-Adapter. It should be a list of length same as number of\n",
            " |                  IP-adapters. Each element should be a tensor of shape `(batch_size, num_images, emb_dim)`. It should\n",
            " |                  contain the negative image embedding if `do_classifier_free_guidance` is set to `True`. If not\n",
            " |                  provided, embeddings are computed from the `ip_adapter_image` input argument.\n",
            " |              output_type (`str`, *optional*, defaults to `\"pil\"`):\n",
            " |                  The output format of the generated image. Choose between `PIL.Image` or `np.array`.\n",
            " |              return_dict (`bool`, *optional*, defaults to `True`):\n",
            " |                  Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n",
            " |                  plain tuple.\n",
            " |              cross_attention_kwargs (`dict`, *optional*):\n",
            " |                  A kwargs dictionary that if specified is passed along to the [`AttentionProcessor`] as defined in\n",
            " |                  [`self.processor`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n",
            " |              guidance_rescale (`float`, *optional*, defaults to 0.0):\n",
            " |                  Guidance rescale factor from [Common Diffusion Noise Schedules and Sample Steps are\n",
            " |                  Flawed](https://arxiv.org/pdf/2305.08891.pdf). Guidance rescale factor should fix overexposure when\n",
            " |                  using zero terminal SNR.\n",
            " |              clip_skip (`int`, *optional*):\n",
            " |                  Number of layers to be skipped from CLIP while computing the prompt embeddings. A value of 1 means that\n",
            " |                  the output of the pre-final layer will be used for computing the prompt embeddings.\n",
            " |              callback_on_step_end (`Callable`, `PipelineCallback`, `MultiPipelineCallbacks`, *optional*):\n",
            " |                  A function or a subclass of `PipelineCallback` or `MultiPipelineCallbacks` that is called at the end of\n",
            " |                  each denoising step during the inference. with the following arguments: `callback_on_step_end(self:\n",
            " |                  DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a\n",
            " |                  list of all tensors as specified by `callback_on_step_end_tensor_inputs`.\n",
            " |              callback_on_step_end_tensor_inputs (`List`, *optional*):\n",
            " |                  The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n",
            " |                  will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n",
            " |                  `._callback_tensor_inputs` attribute of your pipeline class.\n",
            " |      \n",
            " |      \n",
            " |      Examples:\n",
            " |          ```py\n",
            " |          >>> import torch\n",
            " |          >>> from diffusers import StableDiffusionPipeline\n",
            " |      \n",
            " |          >>> pipe = StableDiffusionPipeline.from_pretrained(\n",
            " |          ...     \"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16\n",
            " |          ... )\n",
            " |          >>> pipe = pipe.to(\"cuda\")\n",
            " |      \n",
            " |          >>> prompt = \"a photo of an astronaut riding a horse on mars\"\n",
            " |          >>> image = pipe(prompt).images[0]\n",
            " |          ```\n",
            " |      \n",
            " |      \n",
            " |          Returns:\n",
            " |              [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\n",
            " |                  If `return_dict` is `True`, [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] is returned,\n",
            " |                  otherwise a `tuple` is returned where the first element is a list with the generated images and the\n",
            " |                  second element is a list of `bool`s indicating whether the corresponding generated image contains\n",
            " |                  \"not-safe-for-work\" (nsfw) content.\n",
            " |  \n",
            " |  __init__(self, vae: diffusers.models.autoencoders.autoencoder_kl.AutoencoderKL, text_encoder: transformers.models.clip.modeling_clip.CLIPTextModel, tokenizer: transformers.models.clip.tokenization_clip.CLIPTokenizer, unet: diffusers.models.unets.unet_2d_condition.UNet2DConditionModel, scheduler: diffusers.schedulers.scheduling_utils.KarrasDiffusionSchedulers, safety_checker: diffusers.pipelines.stable_diffusion.safety_checker.StableDiffusionSafetyChecker, feature_extractor: transformers.models.clip.image_processing_clip.CLIPImageProcessor, image_encoder: transformers.models.clip.modeling_clip.CLIPVisionModelWithProjection = None, requires_safety_checker: bool = True)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  check_inputs(self, prompt, height, width, callback_steps, negative_prompt=None, prompt_embeds=None, negative_prompt_embeds=None, ip_adapter_image=None, ip_adapter_image_embeds=None, callback_on_step_end_tensor_inputs=None)\n",
            " |  \n",
            " |  decode_latents(self, latents)\n",
            " |  \n",
            " |  encode_image(self, image, device, num_images_per_prompt, output_hidden_states=None)\n",
            " |  \n",
            " |  encode_prompt(self, prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt=None, prompt_embeds: Optional[torch.Tensor] = None, negative_prompt_embeds: Optional[torch.Tensor] = None, lora_scale: Optional[float] = None, clip_skip: Optional[int] = None)\n",
            " |      Encodes the prompt into text encoder hidden states.\n",
            " |      \n",
            " |      Args:\n",
            " |          prompt (`str` or `List[str]`, *optional*):\n",
            " |              prompt to be encoded\n",
            " |          device: (`torch.device`):\n",
            " |              torch device\n",
            " |          num_images_per_prompt (`int`):\n",
            " |              number of images that should be generated per prompt\n",
            " |          do_classifier_free_guidance (`bool`):\n",
            " |              whether to use classifier free guidance or not\n",
            " |          negative_prompt (`str` or `List[str]`, *optional*):\n",
            " |              The prompt or prompts not to guide the image generation. If not defined, one has to pass\n",
            " |              `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n",
            " |              less than `1`).\n",
            " |          prompt_embeds (`torch.Tensor`, *optional*):\n",
            " |              Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n",
            " |              provided, text embeddings will be generated from `prompt` input argument.\n",
            " |          negative_prompt_embeds (`torch.Tensor`, *optional*):\n",
            " |              Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n",
            " |              weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n",
            " |              argument.\n",
            " |          lora_scale (`float`, *optional*):\n",
            " |              A LoRA scale that will be applied to all LoRA layers of the text encoder if LoRA layers are loaded.\n",
            " |          clip_skip (`int`, *optional*):\n",
            " |              Number of layers to be skipped from CLIP while computing the prompt embeddings. A value of 1 means that\n",
            " |              the output of the pre-final layer will be used for computing the prompt embeddings.\n",
            " |  \n",
            " |  get_guidance_scale_embedding(self, w: torch.Tensor, embedding_dim: int = 512, dtype: torch.dtype = torch.float32) -> torch.Tensor\n",
            " |      See https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298\n",
            " |      \n",
            " |      Args:\n",
            " |          w (`torch.Tensor`):\n",
            " |              Generate embedding vectors with a specified guidance scale to subsequently enrich timestep embeddings.\n",
            " |          embedding_dim (`int`, *optional*, defaults to 512):\n",
            " |              Dimension of the embeddings to generate.\n",
            " |          dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):\n",
            " |              Data type of the generated embeddings.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `torch.Tensor`: Embedding vectors with shape `(len(w), embedding_dim)`.\n",
            " |  \n",
            " |  prepare_extra_step_kwargs(self, generator, eta)\n",
            " |  \n",
            " |  prepare_ip_adapter_image_embeds(self, ip_adapter_image, ip_adapter_image_embeds, device, num_images_per_prompt, do_classifier_free_guidance)\n",
            " |  \n",
            " |  prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None)\n",
            " |  \n",
            " |  run_safety_checker(self, image, device, dtype)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |  \n",
            " |  clip_skip\n",
            " |  \n",
            " |  cross_attention_kwargs\n",
            " |  \n",
            " |  do_classifier_free_guidance\n",
            " |  \n",
            " |  guidance_rescale\n",
            " |  \n",
            " |  guidance_scale\n",
            " |  \n",
            " |  interrupt\n",
            " |  \n",
            " |  num_timesteps\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  model_cpu_offload_seq = 'text_encoder->image_encoder->unet->vae'\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from diffusers.pipelines.pipeline_utils.DiffusionPipeline:\n",
            " |  \n",
            " |  __setattr__(self, name: str, value: Any)\n",
            " |      Implement setattr(self, name, value).\n",
            " |  \n",
            " |  disable_attention_slicing(self)\n",
            " |      Disable sliced attention computation. If `enable_attention_slicing` was previously called, attention is\n",
            " |      computed in one step.\n",
            " |  \n",
            " |  disable_xformers_memory_efficient_attention(self)\n",
            " |      Disable memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/).\n",
            " |  \n",
            " |  enable_attention_slicing(self, slice_size: Union[int, str, NoneType] = 'auto')\n",
            " |      Enable sliced attention computation. When this option is enabled, the attention module splits the input tensor\n",
            " |      in slices to compute attention in several steps. For more than one attention head, the computation is performed\n",
            " |      sequentially over each head. This is useful to save some memory in exchange for a small speed decrease.\n",
            " |      \n",
            " |      <Tip warning={true}>\n",
            " |      \n",
            " |      ⚠️ Don't enable attention slicing if you're already using `scaled_dot_product_attention` (SDPA) from PyTorch\n",
            " |      2.0 or xFormers. These attention computations are already very memory efficient so you won't need to enable\n",
            " |      this function. If you enable attention slicing with SDPA or xFormers, it can lead to serious slow downs!\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Args:\n",
            " |          slice_size (`str` or `int`, *optional*, defaults to `\"auto\"`):\n",
            " |              When `\"auto\"`, halves the input to the attention heads, so attention will be computed in two steps. If\n",
            " |              `\"max\"`, maximum amount of memory will be saved by running only one slice at a time. If a number is\n",
            " |              provided, uses as many slices as `attention_head_dim // slice_size`. In this case, `attention_head_dim`\n",
            " |              must be a multiple of `slice_size`.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> import torch\n",
            " |      >>> from diffusers import StableDiffusionPipeline\n",
            " |      \n",
            " |      >>> pipe = StableDiffusionPipeline.from_pretrained(\n",
            " |      ...     \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
            " |      ...     torch_dtype=torch.float16,\n",
            " |      ...     use_safetensors=True,\n",
            " |      ... )\n",
            " |      \n",
            " |      >>> prompt = \"a photo of an astronaut riding a horse on mars\"\n",
            " |      >>> pipe.enable_attention_slicing()\n",
            " |      >>> image = pipe(prompt).images[0]\n",
            " |      ```\n",
            " |  \n",
            " |  enable_model_cpu_offload(self, gpu_id: Optional[int] = None, device: Union[torch.device, str] = 'cuda')\n",
            " |      Offloads all models to CPU using accelerate, reducing memory usage with a low impact on performance. Compared\n",
            " |      to `enable_sequential_cpu_offload`, this method moves one whole model at a time to the GPU when its `forward`\n",
            " |      method is called, and the model remains in GPU until the next model runs. Memory savings are lower than with\n",
            " |      `enable_sequential_cpu_offload`, but performance is much better due to the iterative execution of the `unet`.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          gpu_id (`int`, *optional*):\n",
            " |              The ID of the accelerator that shall be used in inference. If not specified, it will default to 0.\n",
            " |          device (`torch.Device` or `str`, *optional*, defaults to \"cuda\"):\n",
            " |              The PyTorch device type of the accelerator that shall be used in inference. If not specified, it will\n",
            " |              default to \"cuda\".\n",
            " |  \n",
            " |  enable_sequential_cpu_offload(self, gpu_id: Optional[int] = None, device: Union[torch.device, str] = 'cuda')\n",
            " |      Offloads all models to CPU using 🤗 Accelerate, significantly reducing memory usage. When called, the state\n",
            " |      dicts of all `torch.nn.Module` components (except those in `self._exclude_from_cpu_offload`) are saved to CPU\n",
            " |      and then moved to `torch.device('meta')` and loaded to GPU only when their specific submodule has its `forward`\n",
            " |      method called. Offloading happens on a submodule basis. Memory savings are higher than with\n",
            " |      `enable_model_cpu_offload`, but performance is lower.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          gpu_id (`int`, *optional*):\n",
            " |              The ID of the accelerator that shall be used in inference. If not specified, it will default to 0.\n",
            " |          device (`torch.Device` or `str`, *optional*, defaults to \"cuda\"):\n",
            " |              The PyTorch device type of the accelerator that shall be used in inference. If not specified, it will\n",
            " |              default to \"cuda\".\n",
            " |  \n",
            " |  enable_xformers_memory_efficient_attention(self, attention_op: Optional[Callable] = None)\n",
            " |      Enable memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/). When this\n",
            " |      option is enabled, you should observe lower GPU memory usage and a potential speed up during inference. Speed\n",
            " |      up during training is not guaranteed.\n",
            " |      \n",
            " |      <Tip warning={true}>\n",
            " |      \n",
            " |      ⚠️ When memory efficient attention and sliced attention are both enabled, memory efficient attention takes\n",
            " |      precedent.\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Parameters:\n",
            " |          attention_op (`Callable`, *optional*):\n",
            " |              Override the default `None` operator for use as `op` argument to the\n",
            " |              [`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)\n",
            " |              function of xFormers.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> import torch\n",
            " |      >>> from diffusers import DiffusionPipeline\n",
            " |      >>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp\n",
            " |      \n",
            " |      >>> pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16)\n",
            " |      >>> pipe = pipe.to(\"cuda\")\n",
            " |      >>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n",
            " |      >>> # Workaround for not accepting attention shape using VAE for Flash Attention\n",
            " |      >>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)\n",
            " |      ```\n",
            " |  \n",
            " |  maybe_free_model_hooks(self)\n",
            " |      Method that performs the following:\n",
            " |      - Offloads all components.\n",
            " |      - Removes all model hooks that were added when using `enable_model_cpu_offload`, and then applies them again.\n",
            " |        In case the model has not been offloaded, this function is a no-op.\n",
            " |      - Resets stateful diffusers hooks of denoiser components if they were added with\n",
            " |        [`~hooks.HookRegistry.register_hook`].\n",
            " |      \n",
            " |      Make sure to add this function to the end of the `__call__` function of your pipeline so that it functions\n",
            " |      correctly when applying `enable_model_cpu_offload`.\n",
            " |  \n",
            " |  progress_bar(self, iterable=None, total=None)\n",
            " |  \n",
            " |  register_modules(self, **kwargs)\n",
            " |  \n",
            " |  remove_all_hooks(self)\n",
            " |      Removes all hooks that were added when using `enable_sequential_cpu_offload` or `enable_model_cpu_offload`.\n",
            " |  \n",
            " |  reset_device_map(self)\n",
            " |      Resets the device maps (if any) to None.\n",
            " |  \n",
            " |  save_pretrained(self, save_directory: Union[str, os.PathLike], safe_serialization: bool = True, variant: Optional[str] = None, max_shard_size: Union[int, str, NoneType] = None, push_to_hub: bool = False, **kwargs)\n",
            " |      Save all saveable variables of the pipeline to a directory. A pipeline variable can be saved and loaded if its\n",
            " |      class implements both a save and loading method. The pipeline is easily reloaded using the\n",
            " |      [`~DiffusionPipeline.from_pretrained`] class method.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          save_directory (`str` or `os.PathLike`):\n",
            " |              Directory to save a pipeline to. Will be created if it doesn't exist.\n",
            " |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether to save the model using `safetensors` or the traditional PyTorch way with `pickle`.\n",
            " |          variant (`str`, *optional*):\n",
            " |              If specified, weights are saved in the format `pytorch_model.<variant>.bin`.\n",
            " |          max_shard_size (`int` or `str`, defaults to `None`):\n",
            " |              The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size\n",
            " |              lower than this size. If expressed as a string, needs to be digits followed by a unit (like `\"5GB\"`).\n",
            " |              If expressed as an integer, the unit is bytes. Note that this limit will be decreased after a certain\n",
            " |              period of time (starting from Oct 2024) to allow users to upgrade to the latest version of `diffusers`.\n",
            " |              This is to establish a common default size for this argument across different libraries in the Hugging\n",
            " |              Face ecosystem (`transformers`, and `accelerate`, for example).\n",
            " |          push_to_hub (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n",
            " |              repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n",
            " |              namespace).\n",
            " |      \n",
            " |          kwargs (`Dict[str, Any]`, *optional*):\n",
            " |              Additional keyword arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
            " |  \n",
            " |  set_attention_slice(self, slice_size: Optional[int])\n",
            " |  \n",
            " |  set_progress_bar_config(self, **kwargs)\n",
            " |  \n",
            " |  set_use_memory_efficient_attention_xformers(self, valid: bool, attention_op: Optional[Callable] = None) -> None\n",
            " |  \n",
            " |  to(self, *args, **kwargs) -> Self\n",
            " |      Performs Pipeline dtype and/or device conversion. A torch.dtype and torch.device are inferred from the\n",
            " |      arguments of `self.to(*args, **kwargs).`\n",
            " |      \n",
            " |      <Tip>\n",
            " |      \n",
            " |          If the pipeline already has the correct torch.dtype and torch.device, then it is returned as is. Otherwise,\n",
            " |          the returned pipeline is a copy of self with the desired torch.dtype and torch.device.\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      \n",
            " |      Here are the ways to call `to`:\n",
            " |      \n",
            " |      - `to(dtype, silence_dtype_warnings=False) → DiffusionPipeline` to return a pipeline with the specified\n",
            " |        [`dtype`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype)\n",
            " |      - `to(device, silence_dtype_warnings=False) → DiffusionPipeline` to return a pipeline with the specified\n",
            " |        [`device`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device)\n",
            " |      - `to(device=None, dtype=None, silence_dtype_warnings=False) → DiffusionPipeline` to return a pipeline with the\n",
            " |        specified [`device`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device) and\n",
            " |        [`dtype`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype)\n",
            " |      \n",
            " |      Arguments:\n",
            " |          dtype (`torch.dtype`, *optional*):\n",
            " |              Returns a pipeline with the specified\n",
            " |              [`dtype`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype)\n",
            " |          device (`torch.Device`, *optional*):\n",
            " |              Returns a pipeline with the specified\n",
            " |              [`device`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device)\n",
            " |          silence_dtype_warnings (`str`, *optional*, defaults to `False`):\n",
            " |              Whether to omit warnings if the target `dtype` is not compatible with the target `device`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`DiffusionPipeline`]: The pipeline converted to specified `dtype` and/or `dtype`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from diffusers.pipelines.pipeline_utils.DiffusionPipeline:\n",
            " |  \n",
            " |  download(pretrained_model_name, **kwargs) -> Union[str, os.PathLike]\n",
            " |      Download and cache a PyTorch diffusion pipeline from pretrained pipeline weights.\n",
            " |      \n",
            " |      Parameters:\n",
            " |          pretrained_model_name (`str` or `os.PathLike`, *optional*):\n",
            " |              A string, the *repository id* (for example `CompVis/ldm-text2im-large-256`) of a pretrained pipeline\n",
            " |              hosted on the Hub.\n",
            " |          custom_pipeline (`str`, *optional*):\n",
            " |              Can be either:\n",
            " |      \n",
            " |                  - A string, the *repository id* (for example `CompVis/ldm-text2im-large-256`) of a pretrained\n",
            " |                    pipeline hosted on the Hub. The repository must contain a file called `pipeline.py` that defines\n",
            " |                    the custom pipeline.\n",
            " |      \n",
            " |                  - A string, the *file name* of a community pipeline hosted on GitHub under\n",
            " |                    [Community](https://github.com/huggingface/diffusers/tree/main/examples/community). Valid file\n",
            " |                    names must match the file name and not the pipeline script (`clip_guided_stable_diffusion`\n",
            " |                    instead of `clip_guided_stable_diffusion.py`). Community pipelines are always loaded from the\n",
            " |                    current `main` branch of GitHub.\n",
            " |      \n",
            " |                  - A path to a *directory* (`./my_pipeline_directory/`) containing a custom pipeline. The directory\n",
            " |                    must contain a file called `pipeline.py` that defines the custom pipeline.\n",
            " |      \n",
            " |              <Tip warning={true}>\n",
            " |      \n",
            " |              🧪 This is an experimental feature and may change in the future.\n",
            " |      \n",
            " |              </Tip>\n",
            " |      \n",
            " |              For more information on how to load and create custom pipelines, take a look at [How to contribute a\n",
            " |              community pipeline](https://huggingface.co/docs/diffusers/main/en/using-diffusers/contribute_pipeline).\n",
            " |      \n",
            " |          force_download (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
            " |              cached versions if they exist.\n",
            " |      \n",
            " |          proxies (`Dict[str, str]`, *optional*):\n",
            " |              A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',\n",
            " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
            " |          output_loading_info(`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
            " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether to only load local model weights and configuration files or not. If set to `True`, the model\n",
            " |              won't be downloaded from the Hub.\n",
            " |          token (`str` or *bool*, *optional*):\n",
            " |              The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from\n",
            " |              `diffusers-cli login` (stored in `~/.huggingface`) is used.\n",
            " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
            " |              The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier\n",
            " |              allowed by Git.\n",
            " |          custom_revision (`str`, *optional*, defaults to `\"main\"`):\n",
            " |              The specific model version to use. It can be a branch name, a tag name, or a commit id similar to\n",
            " |              `revision` when loading a custom pipeline from the Hub. It can be a 🤗 Diffusers version when loading a\n",
            " |              custom pipeline from GitHub, otherwise it defaults to `\"main\"` when loading from the Hub.\n",
            " |          mirror (`str`, *optional*):\n",
            " |              Mirror source to resolve accessibility issues if you're downloading a model in China. We do not\n",
            " |              guarantee the timeliness or safety of the source, and you should refer to the mirror site for more\n",
            " |              information.\n",
            " |          variant (`str`, *optional*):\n",
            " |              Load weights from a specified variant filename such as `\"fp16\"` or `\"ema\"`. This is ignored when\n",
            " |              loading `from_flax`.\n",
            " |          dduf_file(`str`, *optional*):\n",
            " |              Load weights from the specified DDUF file.\n",
            " |          use_safetensors (`bool`, *optional*, defaults to `None`):\n",
            " |              If set to `None`, the safetensors weights are downloaded if they're available **and** if the\n",
            " |              safetensors library is installed. If set to `True`, the model is forcibly loaded from safetensors\n",
            " |              weights. If set to `False`, safetensors weights are not loaded.\n",
            " |          use_onnx (`bool`, *optional*, defaults to `False`):\n",
            " |              If set to `True`, ONNX weights will always be downloaded if present. If set to `False`, ONNX weights\n",
            " |              will never be downloaded. By default `use_onnx` defaults to the `_is_onnx` class attribute which is\n",
            " |              `False` for non-ONNX pipelines and `True` for ONNX pipelines. ONNX weights include both files ending\n",
            " |              with `.onnx` and `.pb`.\n",
            " |          trust_remote_code (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to allow for custom pipelines and components defined on the Hub in their own files. This\n",
            " |              option should only be set to `True` for repositories you trust and in which you have read the code, as\n",
            " |              it will execute code present on the Hub on your local machine.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `os.PathLike`:\n",
            " |              A path to the downloaded pipeline.\n",
            " |      \n",
            " |      <Tip>\n",
            " |      \n",
            " |      To use private or [gated models](https://huggingface.co/docs/hub/models-gated#gated-models), log-in with\n",
            " |      `huggingface-cli login`.\n",
            " |      \n",
            " |      </Tip>\n",
            " |  \n",
            " |  from_pipe(pipeline, **kwargs)\n",
            " |      Create a new pipeline from a given pipeline. This method is useful to create a new pipeline from the existing\n",
            " |      pipeline components without reallocating additional memory.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          pipeline (`DiffusionPipeline`):\n",
            " |              The pipeline from which to create a new pipeline.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `DiffusionPipeline`:\n",
            " |              A new pipeline with the same weights and configurations as `pipeline`.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from diffusers import StableDiffusionPipeline, StableDiffusionSAGPipeline\n",
            " |      \n",
            " |      >>> pipe = StableDiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\")\n",
            " |      >>> new_pipe = StableDiffusionSAGPipeline.from_pipe(pipe)\n",
            " |      ```\n",
            " |  \n",
            " |  from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike, NoneType], **kwargs) -> Self\n",
            " |      Instantiate a PyTorch diffusion pipeline from pretrained pipeline weights.\n",
            " |      \n",
            " |      The pipeline is set in evaluation mode (`model.eval()`) by default.\n",
            " |      \n",
            " |      If you get the error message below, you need to finetune the weights for your downstream task:\n",
            " |      \n",
            " |      ```\n",
            " |      Some weights of UNet2DConditionModel were not initialized from the model checkpoint at stable-diffusion-v1-5/stable-diffusion-v1-5 and are newly initialized because the shapes did not match:\n",
            " |      - conv_in.weight: found shape torch.Size([320, 4, 3, 3]) in the checkpoint and torch.Size([320, 9, 3, 3]) in the model instantiated\n",
            " |      You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            " |      ```\n",
            " |      \n",
            " |      Parameters:\n",
            " |          pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n",
            " |              Can be either:\n",
            " |      \n",
            " |                  - A string, the *repo id* (for example `CompVis/ldm-text2im-large-256`) of a pretrained pipeline\n",
            " |                    hosted on the Hub.\n",
            " |                  - A path to a *directory* (for example `./my_pipeline_directory/`) containing pipeline weights\n",
            " |                    saved using\n",
            " |                  [`~DiffusionPipeline.save_pretrained`].\n",
            " |                  - A path to a *directory* (for example `./my_pipeline_directory/`) containing a dduf file\n",
            " |          torch_dtype (`str` or `torch.dtype` or `dict[str, Union[str, torch.dtype]]`, *optional*):\n",
            " |              Override the default `torch.dtype` and load the model with another dtype. If \"auto\" is passed, the\n",
            " |              dtype is automatically derived from the model's weights. To load submodels with different dtype pass a\n",
            " |              `dict` (for example `{'transformer': torch.bfloat16, 'vae': torch.float16}`). Set the default dtype for\n",
            " |              unspecified components with `default` (for example `{'transformer': torch.bfloat16, 'default':\n",
            " |              torch.float16}`). If a component is not specified and no default is set, `torch.float32` is used.\n",
            " |          custom_pipeline (`str`, *optional*):\n",
            " |      \n",
            " |              <Tip warning={true}>\n",
            " |      \n",
            " |              🧪 This is an experimental feature and may change in the future.\n",
            " |      \n",
            " |              </Tip>\n",
            " |      \n",
            " |              Can be either:\n",
            " |      \n",
            " |                  - A string, the *repo id* (for example `hf-internal-testing/diffusers-dummy-pipeline`) of a custom\n",
            " |                    pipeline hosted on the Hub. The repository must contain a file called pipeline.py that defines\n",
            " |                    the custom pipeline.\n",
            " |                  - A string, the *file name* of a community pipeline hosted on GitHub under\n",
            " |                    [Community](https://github.com/huggingface/diffusers/tree/main/examples/community). Valid file\n",
            " |                    names must match the file name and not the pipeline script (`clip_guided_stable_diffusion`\n",
            " |                    instead of `clip_guided_stable_diffusion.py`). Community pipelines are always loaded from the\n",
            " |                    current main branch of GitHub.\n",
            " |                  - A path to a directory (`./my_pipeline_directory/`) containing a custom pipeline. The directory\n",
            " |                    must contain a file called `pipeline.py` that defines the custom pipeline.\n",
            " |      \n",
            " |              For more information on how to load and create custom pipelines, please have a look at [Loading and\n",
            " |              Adding Custom\n",
            " |              Pipelines](https://huggingface.co/docs/diffusers/using-diffusers/custom_pipeline_overview)\n",
            " |          force_download (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
            " |              cached versions if they exist.\n",
            " |          cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
            " |              Path to a directory where a downloaded pretrained model configuration is cached if the standard cache\n",
            " |              is not used.\n",
            " |      \n",
            " |          proxies (`Dict[str, str]`, *optional*):\n",
            " |              A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',\n",
            " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
            " |          output_loading_info(`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
            " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether to only load local model weights and configuration files or not. If set to `True`, the model\n",
            " |              won't be downloaded from the Hub.\n",
            " |          token (`str` or *bool*, *optional*):\n",
            " |              The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from\n",
            " |              `diffusers-cli login` (stored in `~/.huggingface`) is used.\n",
            " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
            " |              The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier\n",
            " |              allowed by Git.\n",
            " |          custom_revision (`str`, *optional*):\n",
            " |              The specific model version to use. It can be a branch name, a tag name, or a commit id similar to\n",
            " |              `revision` when loading a custom pipeline from the Hub. Defaults to the latest stable 🤗 Diffusers\n",
            " |              version.\n",
            " |          mirror (`str`, *optional*):\n",
            " |              Mirror source to resolve accessibility issues if you’re downloading a model in China. We do not\n",
            " |              guarantee the timeliness or safety of the source, and you should refer to the mirror site for more\n",
            " |              information.\n",
            " |          device_map (`str` or `Dict[str, Union[int, str, torch.device]]`, *optional*):\n",
            " |              A map that specifies where each submodule should go. It doesn’t need to be defined for each\n",
            " |              parameter/buffer name; once a given module name is inside, every submodule of it will be sent to the\n",
            " |              same device.\n",
            " |      \n",
            " |              Set `device_map=\"auto\"` to have 🤗 Accelerate automatically compute the most optimized `device_map`. For\n",
            " |              more information about each option see [designing a device\n",
            " |              map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).\n",
            " |          max_memory (`Dict`, *optional*):\n",
            " |              A dictionary device identifier for the maximum memory. Will default to the maximum memory available for\n",
            " |              each GPU and the available CPU RAM if unset.\n",
            " |          offload_folder (`str` or `os.PathLike`, *optional*):\n",
            " |              The path to offload weights if device_map contains the value `\"disk\"`.\n",
            " |          offload_state_dict (`bool`, *optional*):\n",
            " |              If `True`, temporarily offloads the CPU state dict to the hard drive to avoid running out of CPU RAM if\n",
            " |              the weight of the CPU state dict + the biggest shard of the checkpoint does not fit. Defaults to `True`\n",
            " |              when there is some disk offload.\n",
            " |          low_cpu_mem_usage (`bool`, *optional*, defaults to `True` if torch version >= 1.9.0 else `False`):\n",
            " |              Speed up model loading only loading the pretrained weights and not initializing the weights. This also\n",
            " |              tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model.\n",
            " |              Only supported for PyTorch >= 1.9.0. If you are using an older version of PyTorch, setting this\n",
            " |              argument to `True` will raise an error.\n",
            " |          use_safetensors (`bool`, *optional*, defaults to `None`):\n",
            " |              If set to `None`, the safetensors weights are downloaded if they're available **and** if the\n",
            " |              safetensors library is installed. If set to `True`, the model is forcibly loaded from safetensors\n",
            " |              weights. If set to `False`, safetensors weights are not loaded.\n",
            " |          use_onnx (`bool`, *optional*, defaults to `None`):\n",
            " |              If set to `True`, ONNX weights will always be downloaded if present. If set to `False`, ONNX weights\n",
            " |              will never be downloaded. By default `use_onnx` defaults to the `_is_onnx` class attribute which is\n",
            " |              `False` for non-ONNX pipelines and `True` for ONNX pipelines. ONNX weights include both files ending\n",
            " |              with `.onnx` and `.pb`.\n",
            " |          kwargs (remaining dictionary of keyword arguments, *optional*):\n",
            " |              Can be used to overwrite load and saveable variables (the pipeline components of the specific pipeline\n",
            " |              class). The overwritten components are passed directly to the pipelines `__init__` method. See example\n",
            " |              below for more information.\n",
            " |          variant (`str`, *optional*):\n",
            " |              Load weights from a specified variant filename such as `\"fp16\"` or `\"ema\"`. This is ignored when\n",
            " |              loading `from_flax`.\n",
            " |          dduf_file(`str`, *optional*):\n",
            " |              Load weights from the specified dduf file.\n",
            " |      \n",
            " |      <Tip>\n",
            " |      \n",
            " |      To use private or [gated](https://huggingface.co/docs/hub/models-gated#gated-models) models, log-in with\n",
            " |      `huggingface-cli login`.\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from diffusers import DiffusionPipeline\n",
            " |      \n",
            " |      >>> # Download pipeline from huggingface.co and cache.\n",
            " |      >>> pipeline = DiffusionPipeline.from_pretrained(\"CompVis/ldm-text2im-large-256\")\n",
            " |      \n",
            " |      >>> # Download pipeline that requires an authorization token\n",
            " |      >>> # For more information on access tokens, please refer to this section\n",
            " |      >>> # of the documentation](https://huggingface.co/docs/hub/security-tokens)\n",
            " |      >>> pipeline = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\")\n",
            " |      \n",
            " |      >>> # Use a different scheduler\n",
            " |      >>> from diffusers import LMSDiscreteScheduler\n",
            " |      \n",
            " |      >>> scheduler = LMSDiscreteScheduler.from_config(pipeline.scheduler.config)\n",
            " |      >>> pipeline.scheduler = scheduler\n",
            " |      ```\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods inherited from diffusers.pipelines.pipeline_utils.DiffusionPipeline:\n",
            " |  \n",
            " |  numpy_to_pil(images)\n",
            " |      Convert a NumPy image or a batch of images to a PIL image.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from diffusers.pipelines.pipeline_utils.DiffusionPipeline:\n",
            " |  \n",
            " |  components\n",
            " |      The `self.components` property can be useful to run different pipelines with the same weights and\n",
            " |      configurations without reallocating additional memory.\n",
            " |      \n",
            " |      Returns (`dict`):\n",
            " |          A dictionary containing all the modules needed to initialize the pipeline.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from diffusers import (\n",
            " |      ...     StableDiffusionPipeline,\n",
            " |      ...     StableDiffusionImg2ImgPipeline,\n",
            " |      ...     StableDiffusionInpaintPipeline,\n",
            " |      ... )\n",
            " |      \n",
            " |      >>> text2img = StableDiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\")\n",
            " |      >>> img2img = StableDiffusionImg2ImgPipeline(**text2img.components)\n",
            " |      >>> inpaint = StableDiffusionInpaintPipeline(**text2img.components)\n",
            " |      ```\n",
            " |  \n",
            " |  device\n",
            " |      Returns:\n",
            " |          `torch.device`: The torch device on which the pipeline is located.\n",
            " |  \n",
            " |  dtype\n",
            " |      Returns:\n",
            " |          `torch.dtype`: The torch dtype on which the pipeline is located.\n",
            " |  \n",
            " |  name_or_path\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from diffusers.pipelines.pipeline_utils.DiffusionPipeline:\n",
            " |  \n",
            " |  config_name = 'model_index.json'\n",
            " |  \n",
            " |  hf_device_map = None\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from diffusers.configuration_utils.ConfigMixin:\n",
            " |  \n",
            " |  __getattr__(self, name: str) -> Any\n",
            " |      The only reason we overwrite `getattr` here is to gracefully deprecate accessing\n",
            " |      config attributes directly. See https://github.com/huggingface/diffusers/pull/3129\n",
            " |      \n",
            " |      This function is mostly copied from PyTorch's __getattr__ overwrite:\n",
            " |      https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  register_to_config(self, **kwargs)\n",
            " |  \n",
            " |  save_config(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs)\n",
            " |      Save a configuration object to the directory specified in `save_directory` so that it can be reloaded using the\n",
            " |      [`~ConfigMixin.from_config`] class method.\n",
            " |      \n",
            " |      Args:\n",
            " |          save_directory (`str` or `os.PathLike`):\n",
            " |              Directory where the configuration JSON file is saved (will be created if it does not exist).\n",
            " |          push_to_hub (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to push your model to the Hugging Face Hub after saving it. You can specify the\n",
            " |              repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n",
            " |              namespace).\n",
            " |          kwargs (`Dict[str, Any]`, *optional*):\n",
            " |              Additional keyword arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
            " |  \n",
            " |  to_json_file(self, json_file_path: Union[str, os.PathLike])\n",
            " |      Save the configuration instance's parameters to a JSON file.\n",
            " |      \n",
            " |      Args:\n",
            " |          json_file_path (`str` or `os.PathLike`):\n",
            " |              Path to the JSON file to save a configuration instance's parameters.\n",
            " |  \n",
            " |  to_json_string(self) -> str\n",
            " |      Serializes the configuration instance to a JSON string.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `str`:\n",
            " |              String containing all the attributes that make up the configuration instance in JSON format.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from diffusers.configuration_utils.ConfigMixin:\n",
            " |  \n",
            " |  extract_init_dict(config_dict, **kwargs)\n",
            " |  \n",
            " |  from_config(config: Union[diffusers.configuration_utils.FrozenDict, Dict[str, Any]] = None, return_unused_kwargs=False, **kwargs) -> Union[Self, Tuple[Self, Dict[str, Any]]]\n",
            " |      Instantiate a Python class from a config dictionary.\n",
            " |      \n",
            " |      Parameters:\n",
            " |          config (`Dict[str, Any]`):\n",
            " |              A config dictionary from which the Python class is instantiated. Make sure to only load configuration\n",
            " |              files of compatible classes.\n",
            " |          return_unused_kwargs (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether kwargs that are not consumed by the Python class should be returned or not.\n",
            " |          kwargs (remaining dictionary of keyword arguments, *optional*):\n",
            " |              Can be used to update the configuration object (after it is loaded) and initiate the Python class.\n",
            " |              `**kwargs` are passed directly to the underlying scheduler/model's `__init__` method and eventually\n",
            " |              overwrite the same named arguments in `config`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`ModelMixin`] or [`SchedulerMixin`]:\n",
            " |              A model or scheduler object instantiated from a config dictionary.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> from diffusers import DDPMScheduler, DDIMScheduler, PNDMScheduler\n",
            " |      \n",
            " |      >>> # Download scheduler from huggingface.co and cache.\n",
            " |      >>> scheduler = DDPMScheduler.from_pretrained(\"google/ddpm-cifar10-32\")\n",
            " |      \n",
            " |      >>> # Instantiate DDIM scheduler class with same config as DDPM\n",
            " |      >>> scheduler = DDIMScheduler.from_config(scheduler.config)\n",
            " |      \n",
            " |      >>> # Instantiate PNDM scheduler class with same config as DDPM\n",
            " |      >>> scheduler = PNDMScheduler.from_config(scheduler.config)\n",
            " |      ```\n",
            " |  \n",
            " |  get_config_dict(*args, **kwargs)\n",
            " |  \n",
            " |  load_config(pretrained_model_name_or_path: Union[str, os.PathLike], return_unused_kwargs=False, return_commit_hash=False, **kwargs) -> Tuple[Dict[str, Any], Dict[str, Any]]\n",
            " |      Load a model or scheduler configuration.\n",
            " |      \n",
            " |      Parameters:\n",
            " |          pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n",
            " |              Can be either:\n",
            " |      \n",
            " |                  - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on\n",
            " |                    the Hub.\n",
            " |                  - A path to a *directory* (for example `./my_model_directory`) containing model weights saved with\n",
            " |                    [`~ConfigMixin.save_config`].\n",
            " |      \n",
            " |          cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
            " |              Path to a directory where a downloaded pretrained model configuration is cached if the standard cache\n",
            " |              is not used.\n",
            " |          force_download (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
            " |              cached versions if they exist.\n",
            " |          proxies (`Dict[str, str]`, *optional*):\n",
            " |              A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',\n",
            " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
            " |          output_loading_info(`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
            " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether to only load local model weights and configuration files or not. If set to `True`, the model\n",
            " |              won't be downloaded from the Hub.\n",
            " |          token (`str` or *bool*, *optional*):\n",
            " |              The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from\n",
            " |              `diffusers-cli login` (stored in `~/.huggingface`) is used.\n",
            " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
            " |              The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier\n",
            " |              allowed by Git.\n",
            " |          subfolder (`str`, *optional*, defaults to `\"\"`):\n",
            " |              The subfolder location of a model file within a larger model repository on the Hub or locally.\n",
            " |          return_unused_kwargs (`bool`, *optional*, defaults to `False):\n",
            " |              Whether unused keyword arguments of the config are returned.\n",
            " |          return_commit_hash (`bool`, *optional*, defaults to `False):\n",
            " |              Whether the `commit_hash` of the loaded configuration are returned.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `dict`:\n",
            " |              A dictionary of all the parameters stored in a JSON configuration file.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from diffusers.configuration_utils.ConfigMixin:\n",
            " |  \n",
            " |  config\n",
            " |      Returns the config of the class as a frozen dictionary\n",
            " |      \n",
            " |      Returns:\n",
            " |          `Dict[str, Any]`: Config of the class.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from diffusers.configuration_utils.ConfigMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from diffusers.configuration_utils.ConfigMixin:\n",
            " |  \n",
            " |  has_compatibles = False\n",
            " |  \n",
            " |  ignore_for_config = []\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from diffusers.utils.hub_utils.PushToHubMixin:\n",
            " |  \n",
            " |  push_to_hub(self, repo_id: str, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Optional[str] = None, create_pr: bool = False, safe_serialization: bool = True, variant: Optional[str] = None) -> str\n",
            " |      Upload model, scheduler, or pipeline files to the 🤗 Hugging Face Hub.\n",
            " |      \n",
            " |      Parameters:\n",
            " |          repo_id (`str`):\n",
            " |              The name of the repository you want to push your model, scheduler, or pipeline files to. It should\n",
            " |              contain your organization name when pushing to an organization. `repo_id` can also be a path to a local\n",
            " |              directory.\n",
            " |          commit_message (`str`, *optional*):\n",
            " |              Message to commit while pushing. Default to `\"Upload {object}\"`.\n",
            " |          private (`bool`, *optional*):\n",
            " |              Whether to make the repo private. If `None` (default), the repo will be public unless the\n",
            " |              organization's default is private. This value is ignored if the repo already exists.\n",
            " |          token (`str`, *optional*):\n",
            " |              The token to use as HTTP bearer authorization for remote files. The token generated when running\n",
            " |              `huggingface-cli login` (stored in `~/.huggingface`).\n",
            " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to create a PR with the uploaded files or directly commit.\n",
            " |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to convert the model weights to the `safetensors` format.\n",
            " |          variant (`str`, *optional*):\n",
            " |              If specified, weights are saved in the format `pytorch_model.<variant>.bin`.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```python\n",
            " |      from diffusers import UNet2DConditionModel\n",
            " |      \n",
            " |      unet = UNet2DConditionModel.from_pretrained(\"stabilityai/stable-diffusion-2\", subfolder=\"unet\")\n",
            " |      \n",
            " |      # Push the `unet` to your namespace with the name \"my-finetuned-unet\".\n",
            " |      unet.push_to_hub(\"my-finetuned-unet\")\n",
            " |      \n",
            " |      # Push the `unet` to an organization with the name \"my-finetuned-unet\".\n",
            " |      unet.push_to_hub(\"your-org/my-finetuned-unet\")\n",
            " |      ```\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from diffusers.pipelines.pipeline_utils.StableDiffusionMixin:\n",
            " |  \n",
            " |  disable_freeu(self)\n",
            " |      Disables the FreeU mechanism if enabled.\n",
            " |  \n",
            " |  disable_vae_slicing(self)\n",
            " |      Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled, this method will go back to\n",
            " |      computing decoding in one step.\n",
            " |  \n",
            " |  disable_vae_tiling(self)\n",
            " |      Disable tiled VAE decoding. If `enable_vae_tiling` was previously enabled, this method will go back to\n",
            " |      computing decoding in one step.\n",
            " |  \n",
            " |  enable_freeu(self, s1: float, s2: float, b1: float, b2: float)\n",
            " |      Enables the FreeU mechanism as in https://arxiv.org/abs/2309.11497.\n",
            " |      \n",
            " |      The suffixes after the scaling factors represent the stages where they are being applied.\n",
            " |      \n",
            " |      Please refer to the [official repository](https://github.com/ChenyangSi/FreeU) for combinations of the values\n",
            " |      that are known to work well for different pipelines such as Stable Diffusion v1, v2, and Stable Diffusion XL.\n",
            " |      \n",
            " |      Args:\n",
            " |          s1 (`float`):\n",
            " |              Scaling factor for stage 1 to attenuate the contributions of the skip features. This is done to\n",
            " |              mitigate \"oversmoothing effect\" in the enhanced denoising process.\n",
            " |          s2 (`float`):\n",
            " |              Scaling factor for stage 2 to attenuate the contributions of the skip features. This is done to\n",
            " |              mitigate \"oversmoothing effect\" in the enhanced denoising process.\n",
            " |          b1 (`float`): Scaling factor for stage 1 to amplify the contributions of backbone features.\n",
            " |          b2 (`float`): Scaling factor for stage 2 to amplify the contributions of backbone features.\n",
            " |  \n",
            " |  enable_vae_slicing(self)\n",
            " |      Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n",
            " |      compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n",
            " |  \n",
            " |  enable_vae_tiling(self)\n",
            " |      Enable tiled VAE decoding. When this option is enabled, the VAE will split the input tensor into tiles to\n",
            " |      compute decoding and encoding in several steps. This is useful for saving a large amount of memory and to allow\n",
            " |      processing larger images.\n",
            " |  \n",
            " |  fuse_qkv_projections(self, unet: bool = True, vae: bool = True)\n",
            " |      Enables fused QKV projections. For self-attention modules, all projection matrices (i.e., query, key, value)\n",
            " |      are fused. For cross-attention modules, key and value projection matrices are fused.\n",
            " |      \n",
            " |      <Tip warning={true}>\n",
            " |      \n",
            " |      This API is 🧪 experimental.\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Args:\n",
            " |          unet (`bool`, defaults to `True`): To apply fusion on the UNet.\n",
            " |          vae (`bool`, defaults to `True`): To apply fusion on the VAE.\n",
            " |  \n",
            " |  unfuse_qkv_projections(self, unet: bool = True, vae: bool = True)\n",
            " |      Disable QKV projection fusion if enabled.\n",
            " |      \n",
            " |      <Tip warning={true}>\n",
            " |      \n",
            " |      This API is 🧪 experimental.\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Args:\n",
            " |          unet (`bool`, defaults to `True`): To apply fusion on the UNet.\n",
            " |          vae (`bool`, defaults to `True`): To apply fusion on the VAE.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from diffusers.loaders.textual_inversion.TextualInversionLoaderMixin:\n",
            " |  \n",
            " |  load_textual_inversion(self, pretrained_model_name_or_path: Union[str, List[str], Dict[str, torch.Tensor], List[Dict[str, torch.Tensor]]], token: Union[str, List[str], NoneType] = None, tokenizer: Optional[ForwardRef('PreTrainedTokenizer')] = None, text_encoder: Optional[ForwardRef('PreTrainedModel')] = None, **kwargs)\n",
            " |      Load Textual Inversion embeddings into the text encoder of [`StableDiffusionPipeline`] (both 🤗 Diffusers and\n",
            " |      Automatic1111 formats are supported).\n",
            " |      \n",
            " |      Parameters:\n",
            " |          pretrained_model_name_or_path (`str` or `os.PathLike` or `List[str or os.PathLike]` or `Dict` or `List[Dict]`):\n",
            " |              Can be either one of the following or a list of them:\n",
            " |      \n",
            " |                  - A string, the *model id* (for example `sd-concepts-library/low-poly-hd-logos-icons`) of a\n",
            " |                    pretrained model hosted on the Hub.\n",
            " |                  - A path to a *directory* (for example `./my_text_inversion_directory/`) containing the textual\n",
            " |                    inversion weights.\n",
            " |                  - A path to a *file* (for example `./my_text_inversions.pt`) containing textual inversion weights.\n",
            " |                  - A [torch state\n",
            " |                    dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict).\n",
            " |      \n",
            " |          token (`str` or `List[str]`, *optional*):\n",
            " |              Override the token to use for the textual inversion weights. If `pretrained_model_name_or_path` is a\n",
            " |              list, then `token` must also be a list of equal length.\n",
            " |          text_encoder ([`~transformers.CLIPTextModel`], *optional*):\n",
            " |              Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).\n",
            " |              If not specified, function will take self.tokenizer.\n",
            " |          tokenizer ([`~transformers.CLIPTokenizer`], *optional*):\n",
            " |              A `CLIPTokenizer` to tokenize text. If not specified, function will take self.tokenizer.\n",
            " |          weight_name (`str`, *optional*):\n",
            " |              Name of a custom weight file. This should be used when:\n",
            " |      \n",
            " |                  - The saved textual inversion file is in 🤗 Diffusers format, but was saved under a specific weight\n",
            " |                    name such as `text_inv.bin`.\n",
            " |                  - The saved textual inversion file is in the Automatic1111 format.\n",
            " |          cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
            " |              Path to a directory where a downloaded pretrained model configuration is cached if the standard cache\n",
            " |              is not used.\n",
            " |          force_download (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
            " |              cached versions if they exist.\n",
            " |      \n",
            " |          proxies (`Dict[str, str]`, *optional*):\n",
            " |              A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',\n",
            " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
            " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether to only load local model weights and configuration files or not. If set to `True`, the model\n",
            " |              won't be downloaded from the Hub.\n",
            " |          hf_token (`str` or *bool*, *optional*):\n",
            " |              The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from\n",
            " |              `diffusers-cli login` (stored in `~/.huggingface`) is used.\n",
            " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
            " |              The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier\n",
            " |              allowed by Git.\n",
            " |          subfolder (`str`, *optional*, defaults to `\"\"`):\n",
            " |              The subfolder location of a model file within a larger model repository on the Hub or locally.\n",
            " |          mirror (`str`, *optional*):\n",
            " |              Mirror source to resolve accessibility issues if you're downloading a model in China. We do not\n",
            " |              guarantee the timeliness or safety of the source, and you should refer to the mirror site for more\n",
            " |              information.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      To load a Textual Inversion embedding vector in 🤗 Diffusers format:\n",
            " |      \n",
            " |      ```py\n",
            " |      from diffusers import StableDiffusionPipeline\n",
            " |      import torch\n",
            " |      \n",
            " |      model_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
            " |      pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
            " |      \n",
            " |      pipe.load_textual_inversion(\"sd-concepts-library/cat-toy\")\n",
            " |      \n",
            " |      prompt = \"A <cat-toy> backpack\"\n",
            " |      \n",
            " |      image = pipe(prompt, num_inference_steps=50).images[0]\n",
            " |      image.save(\"cat-backpack.png\")\n",
            " |      ```\n",
            " |      \n",
            " |      To load a Textual Inversion embedding vector in Automatic1111 format, make sure to download the vector first\n",
            " |      (for example from [civitAI](https://civitai.com/models/3036?modelVersionId=9857)) and then load the vector\n",
            " |      locally:\n",
            " |      \n",
            " |      ```py\n",
            " |      from diffusers import StableDiffusionPipeline\n",
            " |      import torch\n",
            " |      \n",
            " |      model_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
            " |      pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
            " |      \n",
            " |      pipe.load_textual_inversion(\"./charturnerv2.pt\", token=\"charturnerv2\")\n",
            " |      \n",
            " |      prompt = \"charturnerv2, multiple views of the same character in the same outfit, a character turnaround of a woman wearing a black jacket and red shirt, best quality, intricate details.\"\n",
            " |      \n",
            " |      image = pipe(prompt, num_inference_steps=50).images[0]\n",
            " |      image.save(\"character.png\")\n",
            " |      ```\n",
            " |  \n",
            " |  maybe_convert_prompt(self, prompt: Union[str, List[str]], tokenizer: 'PreTrainedTokenizer')\n",
            " |      Processes prompts that include a special token corresponding to a multi-vector textual inversion embedding to\n",
            " |      be replaced with multiple special tokens each corresponding to one of the vectors. If the prompt has no textual\n",
            " |      inversion token or if the textual inversion token is a single vector, the input prompt is returned.\n",
            " |      \n",
            " |      Parameters:\n",
            " |          prompt (`str` or list of `str`):\n",
            " |              The prompt or prompts to guide the image generation.\n",
            " |          tokenizer (`PreTrainedTokenizer`):\n",
            " |              The tokenizer responsible for encoding the prompt into input tokens.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `str` or list of `str`: The converted prompt\n",
            " |  \n",
            " |  unload_textual_inversion(self, tokens: Union[str, List[str], NoneType] = None, tokenizer: Optional[ForwardRef('PreTrainedTokenizer')] = None, text_encoder: Optional[ForwardRef('PreTrainedModel')] = None)\n",
            " |      Unload Textual Inversion embeddings from the text encoder of [`StableDiffusionPipeline`]\n",
            " |      \n",
            " |      Example:\n",
            " |      ```py\n",
            " |      from diffusers import AutoPipelineForText2Image\n",
            " |      import torch\n",
            " |      \n",
            " |      pipeline = AutoPipelineForText2Image.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\")\n",
            " |      \n",
            " |      # Example 1\n",
            " |      pipeline.load_textual_inversion(\"sd-concepts-library/gta5-artwork\")\n",
            " |      pipeline.load_textual_inversion(\"sd-concepts-library/moeb-style\")\n",
            " |      \n",
            " |      # Remove all token embeddings\n",
            " |      pipeline.unload_textual_inversion()\n",
            " |      \n",
            " |      # Example 2\n",
            " |      pipeline.load_textual_inversion(\"sd-concepts-library/moeb-style\")\n",
            " |      pipeline.load_textual_inversion(\"sd-concepts-library/gta5-artwork\")\n",
            " |      \n",
            " |      # Remove just one token\n",
            " |      pipeline.unload_textual_inversion(\"<moe-bius>\")\n",
            " |      \n",
            " |      # Example 3: unload from SDXL\n",
            " |      pipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\")\n",
            " |      embedding_path = hf_hub_download(\n",
            " |          repo_id=\"linoyts/web_y2k\", filename=\"web_y2k_emb.safetensors\", repo_type=\"model\"\n",
            " |      )\n",
            " |      \n",
            " |      # load embeddings to the text encoders\n",
            " |      state_dict = load_file(embedding_path)\n",
            " |      \n",
            " |      # load embeddings of text_encoder 1 (CLIP ViT-L/14)\n",
            " |      pipeline.load_textual_inversion(\n",
            " |          state_dict[\"clip_l\"],\n",
            " |          tokens=[\"<s0>\", \"<s1>\"],\n",
            " |          text_encoder=pipeline.text_encoder,\n",
            " |          tokenizer=pipeline.tokenizer,\n",
            " |      )\n",
            " |      # load embeddings of text_encoder 2 (CLIP ViT-G/14)\n",
            " |      pipeline.load_textual_inversion(\n",
            " |          state_dict[\"clip_g\"],\n",
            " |          tokens=[\"<s0>\", \"<s1>\"],\n",
            " |          text_encoder=pipeline.text_encoder_2,\n",
            " |          tokenizer=pipeline.tokenizer_2,\n",
            " |      )\n",
            " |      \n",
            " |      # Unload explicitly from both text encoders and tokenizers\n",
            " |      pipeline.unload_textual_inversion(\n",
            " |          tokens=[\"<s0>\", \"<s1>\"], text_encoder=pipeline.text_encoder, tokenizer=pipeline.tokenizer\n",
            " |      )\n",
            " |      pipeline.unload_textual_inversion(\n",
            " |          tokens=[\"<s0>\", \"<s1>\"], text_encoder=pipeline.text_encoder_2, tokenizer=pipeline.tokenizer_2\n",
            " |      )\n",
            " |      ```\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin:\n",
            " |  \n",
            " |  fuse_lora(self, components: List[str] = ['unet', 'text_encoder'], lora_scale: float = 1.0, safe_fusing: bool = False, adapter_names: Optional[List[str]] = None, **kwargs)\n",
            " |      Fuses the LoRA parameters into the original parameters of the corresponding blocks.\n",
            " |      \n",
            " |      <Tip warning={true}>\n",
            " |      \n",
            " |      This is an experimental API.\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Args:\n",
            " |          components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.\n",
            " |          lora_scale (`float`, defaults to 1.0):\n",
            " |              Controls how much to influence the outputs with the LoRA parameters.\n",
            " |          safe_fusing (`bool`, defaults to `False`):\n",
            " |              Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.\n",
            " |          adapter_names (`List[str]`, *optional*):\n",
            " |              Adapter names to be used for fusing. If nothing is passed, all active adapters will be fused.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      from diffusers import DiffusionPipeline\n",
            " |      import torch\n",
            " |      \n",
            " |      pipeline = DiffusionPipeline.from_pretrained(\n",
            " |          \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16\n",
            " |      ).to(\"cuda\")\n",
            " |      pipeline.load_lora_weights(\"nerijs/pixel-art-xl\", weight_name=\"pixel-art-xl.safetensors\", adapter_name=\"pixel\")\n",
            " |      pipeline.fuse_lora(lora_scale=0.7)\n",
            " |      ```\n",
            " |  \n",
            " |  load_lora_weights(self, pretrained_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]], adapter_name=None, hotswap: bool = False, **kwargs)\n",
            " |      Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.unet` and\n",
            " |      `self.text_encoder`.\n",
            " |      \n",
            " |      All kwargs are forwarded to `self.lora_state_dict`.\n",
            " |      \n",
            " |      See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`] for more details on how the state dict is\n",
            " |      loaded.\n",
            " |      \n",
            " |      See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_unet`] for more details on how the state dict is\n",
            " |      loaded into `self.unet`.\n",
            " |      \n",
            " |      See [`~loaders.StableDiffusionLoraLoaderMixin.load_lora_into_text_encoder`] for more details on how the state\n",
            " |      dict is loaded into `self.text_encoder`.\n",
            " |      \n",
            " |      Parameters:\n",
            " |          pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):\n",
            " |              See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].\n",
            " |          adapter_name (`str`, *optional*):\n",
            " |              Adapter name to be used for referencing the loaded adapter model. If not specified, it will use\n",
            " |              `default_{i}` where i is the total number of adapters being loaded.\n",
            " |          low_cpu_mem_usage (`bool`, *optional*):\n",
            " |              Speed up model loading by only loading the pretrained LoRA weights and not initializing the random\n",
            " |              weights.\n",
            " |          hotswap : (`bool`, *optional*)\n",
            " |              Defaults to `False`. Whether to substitute an existing (LoRA) adapter with the newly loaded adapter\n",
            " |              in-place. This means that, instead of loading an additional adapter, this will take the existing\n",
            " |              adapter weights and replace them with the weights of the new adapter. This can be faster and more\n",
            " |              memory efficient. However, the main advantage of hotswapping is that when the model is compiled with\n",
            " |              torch.compile, loading the new adapter does not require recompilation of the model. When using\n",
            " |              hotswapping, the passed `adapter_name` should be the name of an already loaded adapter.\n",
            " |      \n",
            " |              If the new adapter and the old adapter have different ranks and/or LoRA alphas (i.e. scaling), you need\n",
            " |              to call an additional method before loading the adapter:\n",
            " |      \n",
            " |              ```py\n",
            " |              pipeline = ...  # load diffusers pipeline\n",
            " |              max_rank = ...  # the highest rank among all LoRAs that you want to load\n",
            " |              # call *before* compiling and loading the LoRA adapter\n",
            " |              pipeline.enable_lora_hotswap(target_rank=max_rank)\n",
            " |              pipeline.load_lora_weights(file_name)\n",
            " |              # optionally compile the model now\n",
            " |              ```\n",
            " |      \n",
            " |              Note that hotswapping adapters of the text encoder is not yet supported. There are some further\n",
            " |              limitations to this technique, which are documented here:\n",
            " |              https://huggingface.co/docs/peft/main/en/package_reference/hotswap\n",
            " |          kwargs (`dict`, *optional*):\n",
            " |              See [`~loaders.StableDiffusionLoraLoaderMixin.lora_state_dict`].\n",
            " |  \n",
            " |  unfuse_lora(self, components: List[str] = ['unet', 'text_encoder'], **kwargs)\n",
            " |      Reverses the effect of\n",
            " |      [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraBaseMixin.fuse_lora).\n",
            " |      \n",
            " |      <Tip warning={true}>\n",
            " |      \n",
            " |      This is an experimental API.\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Args:\n",
            " |          components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.\n",
            " |          unfuse_unet (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.\n",
            " |          unfuse_text_encoder (`bool`, defaults to `True`):\n",
            " |              Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn't monkey-patched with the\n",
            " |              LoRA parameters then it won't have any effect.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin:\n",
            " |  \n",
            " |  load_lora_into_text_encoder(state_dict, network_alphas, text_encoder, prefix=None, lora_scale=1.0, adapter_name=None, _pipeline=None, low_cpu_mem_usage=False, hotswap: bool = False)\n",
            " |      This will load the LoRA layers specified in `state_dict` into `text_encoder`\n",
            " |      \n",
            " |      Parameters:\n",
            " |          state_dict (`dict`):\n",
            " |              A standard state dict containing the lora layer parameters. The key should be prefixed with an\n",
            " |              additional `text_encoder` to distinguish between unet lora layers.\n",
            " |          network_alphas (`Dict[str, float]`):\n",
            " |              The value of the network alpha used for stable learning and preventing underflow. This value has the\n",
            " |              same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this\n",
            " |              link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).\n",
            " |          text_encoder (`CLIPTextModel`):\n",
            " |              The text encoder model to load the LoRA layers into.\n",
            " |          prefix (`str`):\n",
            " |              Expected prefix of the `text_encoder` in the `state_dict`.\n",
            " |          lora_scale (`float`):\n",
            " |              How much to scale the output of the lora linear layer before it is added with the output of the regular\n",
            " |              lora layer.\n",
            " |          adapter_name (`str`, *optional*):\n",
            " |              Adapter name to be used for referencing the loaded adapter model. If not specified, it will use\n",
            " |              `default_{i}` where i is the total number of adapters being loaded.\n",
            " |          low_cpu_mem_usage (`bool`, *optional*):\n",
            " |              Speed up model loading by only loading the pretrained LoRA weights and not initializing the random\n",
            " |              weights.\n",
            " |          hotswap : (`bool`, *optional*)\n",
            " |              Defaults to `False`. Whether to substitute an existing (LoRA) adapter with the newly loaded adapter\n",
            " |              in-place. This means that, instead of loading an additional adapter, this will take the existing\n",
            " |              adapter weights and replace them with the weights of the new adapter. This can be faster and more\n",
            " |              memory efficient. However, the main advantage of hotswapping is that when the model is compiled with\n",
            " |              torch.compile, loading the new adapter does not require recompilation of the model. When using\n",
            " |              hotswapping, the passed `adapter_name` should be the name of an already loaded adapter.\n",
            " |      \n",
            " |              If the new adapter and the old adapter have different ranks and/or LoRA alphas (i.e. scaling), you need\n",
            " |              to call an additional method before loading the adapter:\n",
            " |      \n",
            " |              ```py\n",
            " |              pipeline = ...  # load diffusers pipeline\n",
            " |              max_rank = ...  # the highest rank among all LoRAs that you want to load\n",
            " |              # call *before* compiling and loading the LoRA adapter\n",
            " |              pipeline.enable_lora_hotswap(target_rank=max_rank)\n",
            " |              pipeline.load_lora_weights(file_name)\n",
            " |              # optionally compile the model now\n",
            " |              ```\n",
            " |      \n",
            " |              Note that hotswapping adapters of the text encoder is not yet supported. There are some further\n",
            " |              limitations to this technique, which are documented here:\n",
            " |              https://huggingface.co/docs/peft/main/en/package_reference/hotswap\n",
            " |  \n",
            " |  load_lora_into_unet(state_dict, network_alphas, unet, adapter_name=None, _pipeline=None, low_cpu_mem_usage=False, hotswap: bool = False)\n",
            " |      This will load the LoRA layers specified in `state_dict` into `unet`.\n",
            " |      \n",
            " |      Parameters:\n",
            " |          state_dict (`dict`):\n",
            " |              A standard state dict containing the lora layer parameters. The keys can either be indexed directly\n",
            " |              into the unet or prefixed with an additional `unet` which can be used to distinguish between text\n",
            " |              encoder lora layers.\n",
            " |          network_alphas (`Dict[str, float]`):\n",
            " |              The value of the network alpha used for stable learning and preventing underflow. This value has the\n",
            " |              same meaning as the `--network_alpha` option in the kohya-ss trainer script. Refer to [this\n",
            " |              link](https://github.com/darkstorm2150/sd-scripts/blob/main/docs/train_network_README-en.md#execute-learning).\n",
            " |          unet (`UNet2DConditionModel`):\n",
            " |              The UNet model to load the LoRA layers into.\n",
            " |          adapter_name (`str`, *optional*):\n",
            " |              Adapter name to be used for referencing the loaded adapter model. If not specified, it will use\n",
            " |              `default_{i}` where i is the total number of adapters being loaded.\n",
            " |          low_cpu_mem_usage (`bool`, *optional*):\n",
            " |              Speed up model loading only loading the pretrained LoRA weights and not initializing the random\n",
            " |              weights.\n",
            " |          hotswap : (`bool`, *optional*)\n",
            " |              Defaults to `False`. Whether to substitute an existing (LoRA) adapter with the newly loaded adapter\n",
            " |              in-place. This means that, instead of loading an additional adapter, this will take the existing\n",
            " |              adapter weights and replace them with the weights of the new adapter. This can be faster and more\n",
            " |              memory efficient. However, the main advantage of hotswapping is that when the model is compiled with\n",
            " |              torch.compile, loading the new adapter does not require recompilation of the model. When using\n",
            " |              hotswapping, the passed `adapter_name` should be the name of an already loaded adapter.\n",
            " |      \n",
            " |              If the new adapter and the old adapter have different ranks and/or LoRA alphas (i.e. scaling), you need\n",
            " |              to call an additional method before loading the adapter:\n",
            " |      \n",
            " |              ```py\n",
            " |              pipeline = ...  # load diffusers pipeline\n",
            " |              max_rank = ...  # the highest rank among all LoRAs that you want to load\n",
            " |              # call *before* compiling and loading the LoRA adapter\n",
            " |              pipeline.enable_lora_hotswap(target_rank=max_rank)\n",
            " |              pipeline.load_lora_weights(file_name)\n",
            " |              # optionally compile the model now\n",
            " |              ```\n",
            " |      \n",
            " |              Note that hotswapping adapters of the text encoder is not yet supported. There are some further\n",
            " |              limitations to this technique, which are documented here:\n",
            " |              https://huggingface.co/docs/peft/main/en/package_reference/hotswap\n",
            " |  \n",
            " |  lora_state_dict(pretrained_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]], **kwargs)\n",
            " |      Return state dict for lora weights and the network alphas.\n",
            " |      \n",
            " |      <Tip warning={true}>\n",
            " |      \n",
            " |      We support loading A1111 formatted LoRA checkpoints in a limited capacity.\n",
            " |      \n",
            " |      This function is experimental and might change in the future.\n",
            " |      \n",
            " |      </Tip>\n",
            " |      \n",
            " |      Parameters:\n",
            " |          pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):\n",
            " |              Can be either:\n",
            " |      \n",
            " |                  - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on\n",
            " |                    the Hub.\n",
            " |                  - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved\n",
            " |                    with [`ModelMixin.save_pretrained`].\n",
            " |                  - A [torch state\n",
            " |                    dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict).\n",
            " |      \n",
            " |          cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
            " |              Path to a directory where a downloaded pretrained model configuration is cached if the standard cache\n",
            " |              is not used.\n",
            " |          force_download (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
            " |              cached versions if they exist.\n",
            " |      \n",
            " |          proxies (`Dict[str, str]`, *optional*):\n",
            " |              A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',\n",
            " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
            " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether to only load local model weights and configuration files or not. If set to `True`, the model\n",
            " |              won't be downloaded from the Hub.\n",
            " |          token (`str` or *bool*, *optional*):\n",
            " |              The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from\n",
            " |              `diffusers-cli login` (stored in `~/.huggingface`) is used.\n",
            " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
            " |              The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier\n",
            " |              allowed by Git.\n",
            " |          subfolder (`str`, *optional*, defaults to `\"\"`):\n",
            " |              The subfolder location of a model file within a larger model repository on the Hub or locally.\n",
            " |          weight_name (`str`, *optional*, defaults to None):\n",
            " |              Name of the serialized state dict file.\n",
            " |  \n",
            " |  save_lora_weights(save_directory: Union[str, os.PathLike], unet_lora_layers: Dict[str, Union[torch.nn.modules.module.Module, torch.Tensor]] = None, text_encoder_lora_layers: Dict[str, torch.nn.modules.module.Module] = None, is_main_process: bool = True, weight_name: str = None, save_function: Callable = None, safe_serialization: bool = True)\n",
            " |      Save the LoRA parameters corresponding to the UNet and text encoder.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          save_directory (`str` or `os.PathLike`):\n",
            " |              Directory to save LoRA parameters to. Will be created if it doesn't exist.\n",
            " |          unet_lora_layers (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`):\n",
            " |              State dict of the LoRA layers corresponding to the `unet`.\n",
            " |          text_encoder_lora_layers (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`):\n",
            " |              State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text\n",
            " |              encoder LoRA state dict because it comes from 🤗 Transformers.\n",
            " |          is_main_process (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether the process calling this is the main process or not. Useful during distributed training and you\n",
            " |              need to call this function on all processes. In this case, set `is_main_process=True` only on the main\n",
            " |              process to avoid race conditions.\n",
            " |          save_function (`Callable`):\n",
            " |              The function to use to save the state dictionary. Useful during distributed training when you need to\n",
            " |              replace `torch.save` with another method. Can be configured with the environment variable\n",
            " |              `DIFFUSERS_SAVE_MODE`.\n",
            " |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether to save the model using `safetensors` or the traditional PyTorch way with `pickle`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from diffusers.loaders.lora_pipeline.StableDiffusionLoraLoaderMixin:\n",
            " |  \n",
            " |  text_encoder_name = 'text_encoder'\n",
            " |  \n",
            " |  unet_name = 'unet'\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from diffusers.loaders.lora_base.LoraBaseMixin:\n",
            " |  \n",
            " |  delete_adapters(self, adapter_names: Union[List[str], str])\n",
            " |      Args:\n",
            " |      Deletes the LoRA layers of `adapter_name` for the unet and text-encoder(s).\n",
            " |          adapter_names (`Union[List[str], str]`):\n",
            " |              The names of the adapter to delete. Can be a single string or a list of strings\n",
            " |  \n",
            " |  disable_lora(self)\n",
            " |  \n",
            " |  enable_lora(self)\n",
            " |  \n",
            " |  enable_lora_hotswap(self, **kwargs) -> None\n",
            " |      Enables the possibility to hotswap LoRA adapters.\n",
            " |      \n",
            " |      Calling this method is only required when hotswapping adapters and if the model is compiled or if the ranks of\n",
            " |      the loaded adapters differ.\n",
            " |      \n",
            " |      Args:\n",
            " |          target_rank (`int`):\n",
            " |              The highest rank among all the adapters that will be loaded.\n",
            " |          check_compiled (`str`, *optional*, defaults to `\"error\"`):\n",
            " |              How to handle the case when the model is already compiled, which should generally be avoided. The\n",
            " |              options are:\n",
            " |                - \"error\" (default): raise an error\n",
            " |                - \"warn\": issue a warning\n",
            " |                - \"ignore\": do nothing\n",
            " |  \n",
            " |  get_active_adapters(self) -> List[str]\n",
            " |      Gets the list of the current active adapters.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      from diffusers import DiffusionPipeline\n",
            " |      \n",
            " |      pipeline = DiffusionPipeline.from_pretrained(\n",
            " |          \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
            " |      ).to(\"cuda\")\n",
            " |      pipeline.load_lora_weights(\"CiroN2022/toy-face\", weight_name=\"toy_face_sdxl.safetensors\", adapter_name=\"toy\")\n",
            " |      pipeline.get_active_adapters()\n",
            " |      ```\n",
            " |  \n",
            " |  get_list_adapters(self) -> Dict[str, List[str]]\n",
            " |      Gets the current list of all available adapters in the pipeline.\n",
            " |  \n",
            " |  set_adapters(self, adapter_names: Union[List[str], str], adapter_weights: Union[float, Dict, List[float], List[Dict], NoneType] = None)\n",
            " |  \n",
            " |  set_lora_device(self, adapter_names: List[str], device: Union[torch.device, str, int]) -> None\n",
            " |      Moves the LoRAs listed in `adapter_names` to a target device. Useful for offloading the LoRA to the CPU in case\n",
            " |      you want to load multiple adapters and free some GPU memory.\n",
            " |      \n",
            " |      Args:\n",
            " |          adapter_names (`List[str]`):\n",
            " |              List of adapters to send device to.\n",
            " |          device (`Union[torch.device, str, int]`):\n",
            " |              Device to send the adapters to. Can be either a torch device, a str or an integer.\n",
            " |  \n",
            " |  unload_lora_weights(self)\n",
            " |      Unloads the LoRA parameters.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> # Assuming `pipeline` is already loaded with the LoRA parameters.\n",
            " |      >>> pipeline.unload_lora_weights()\n",
            " |      >>> ...\n",
            " |      ```\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods inherited from diffusers.loaders.lora_base.LoraBaseMixin:\n",
            " |  \n",
            " |  pack_weights(layers, prefix)\n",
            " |  \n",
            " |  write_lora_layers(state_dict: Dict[str, torch.Tensor], save_directory: str, is_main_process: bool, weight_name: str, save_function: Callable, safe_serialization: bool)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from diffusers.loaders.lora_base.LoraBaseMixin:\n",
            " |  \n",
            " |  lora_scale\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from diffusers.loaders.lora_base.LoraBaseMixin:\n",
            " |  \n",
            " |  num_fused_loras = 0\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from diffusers.loaders.ip_adapter.IPAdapterMixin:\n",
            " |  \n",
            " |  load_ip_adapter(self, pretrained_model_name_or_path_or_dict: Union[str, List[str], Dict[str, torch.Tensor]], subfolder: Union[str, List[str]], weight_name: Union[str, List[str]], image_encoder_folder: Optional[str] = 'image_encoder', **kwargs)\n",
            " |      Parameters:\n",
            " |          pretrained_model_name_or_path_or_dict (`str` or `List[str]` or `os.PathLike` or `List[os.PathLike]` or `dict` or `List[dict]`):\n",
            " |              Can be either:\n",
            " |      \n",
            " |                  - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on\n",
            " |                    the Hub.\n",
            " |                  - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved\n",
            " |                    with [`ModelMixin.save_pretrained`].\n",
            " |                  - A [torch state\n",
            " |                    dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict).\n",
            " |          subfolder (`str` or `List[str]`):\n",
            " |              The subfolder location of a model file within a larger model repository on the Hub or locally. If a\n",
            " |              list is passed, it should have the same length as `weight_name`.\n",
            " |          weight_name (`str` or `List[str]`):\n",
            " |              The name of the weight file to load. If a list is passed, it should have the same length as\n",
            " |              `subfolder`.\n",
            " |          image_encoder_folder (`str`, *optional*, defaults to `image_encoder`):\n",
            " |              The subfolder location of the image encoder within a larger model repository on the Hub or locally.\n",
            " |              Pass `None` to not load the image encoder. If the image encoder is located in a folder inside\n",
            " |              `subfolder`, you only need to pass the name of the folder that contains image encoder weights, e.g.\n",
            " |              `image_encoder_folder=\"image_encoder\"`. If the image encoder is located in a folder other than\n",
            " |              `subfolder`, you should pass the path to the folder that contains image encoder weights, for example,\n",
            " |              `image_encoder_folder=\"different_subfolder/image_encoder\"`.\n",
            " |          cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
            " |              Path to a directory where a downloaded pretrained model configuration is cached if the standard cache\n",
            " |              is not used.\n",
            " |          force_download (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
            " |              cached versions if they exist.\n",
            " |      \n",
            " |          proxies (`Dict[str, str]`, *optional*):\n",
            " |              A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',\n",
            " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
            " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether to only load local model weights and configuration files or not. If set to `True`, the model\n",
            " |              won't be downloaded from the Hub.\n",
            " |          token (`str` or *bool*, *optional*):\n",
            " |              The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from\n",
            " |              `diffusers-cli login` (stored in `~/.huggingface`) is used.\n",
            " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
            " |              The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier\n",
            " |              allowed by Git.\n",
            " |          low_cpu_mem_usage (`bool`, *optional*, defaults to `True` if torch version >= 1.9.0 else `False`):\n",
            " |              Speed up model loading only loading the pretrained weights and not initializing the weights. This also\n",
            " |              tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model.\n",
            " |              Only supported for PyTorch >= 1.9.0. If you are using an older version of PyTorch, setting this\n",
            " |              argument to `True` will raise an error.\n",
            " |  \n",
            " |  set_ip_adapter_scale(self, scale)\n",
            " |      Set IP-Adapter scales per-transformer block. Input `scale` could be a single config or a list of configs for\n",
            " |      granular control over each IP-Adapter behavior. A config can be a float or a dictionary.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      # To use original IP-Adapter\n",
            " |      scale = 1.0\n",
            " |      pipeline.set_ip_adapter_scale(scale)\n",
            " |      \n",
            " |      # To use style block only\n",
            " |      scale = {\n",
            " |          \"up\": {\"block_0\": [0.0, 1.0, 0.0]},\n",
            " |      }\n",
            " |      pipeline.set_ip_adapter_scale(scale)\n",
            " |      \n",
            " |      # To use style+layout blocks\n",
            " |      scale = {\n",
            " |          \"down\": {\"block_2\": [0.0, 1.0]},\n",
            " |          \"up\": {\"block_0\": [0.0, 1.0, 0.0]},\n",
            " |      }\n",
            " |      pipeline.set_ip_adapter_scale(scale)\n",
            " |      \n",
            " |      # To use style and layout from 2 reference images\n",
            " |      scales = [{\"down\": {\"block_2\": [0.0, 1.0]}}, {\"up\": {\"block_0\": [0.0, 1.0, 0.0]}}]\n",
            " |      pipeline.set_ip_adapter_scale(scales)\n",
            " |      ```\n",
            " |  \n",
            " |  unload_ip_adapter(self)\n",
            " |      Unloads the IP Adapter weights\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> # Assuming `pipeline` is already loaded with the IP Adapter weights.\n",
            " |      >>> pipeline.unload_ip_adapter()\n",
            " |      >>> ...\n",
            " |      ```\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from diffusers.loaders.single_file.FromSingleFileMixin:\n",
            " |  \n",
            " |  from_single_file(pretrained_model_link_or_path, **kwargs) -> Self\n",
            " |      Instantiate a [`DiffusionPipeline`] from pretrained pipeline weights saved in the `.ckpt` or `.safetensors`\n",
            " |      format. The pipeline is set in evaluation mode (`model.eval()`) by default.\n",
            " |      \n",
            " |      Parameters:\n",
            " |          pretrained_model_link_or_path (`str` or `os.PathLike`, *optional*):\n",
            " |              Can be either:\n",
            " |                  - A link to the `.ckpt` file (for example\n",
            " |                    `\"https://huggingface.co/<repo_id>/blob/main/<path_to_file>.ckpt\"`) on the Hub.\n",
            " |                  - A path to a *file* containing all pipeline weights.\n",
            " |          torch_dtype (`str` or `torch.dtype`, *optional*):\n",
            " |              Override the default `torch.dtype` and load the model with another dtype.\n",
            " |          force_download (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
            " |              cached versions if they exist.\n",
            " |          cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
            " |              Path to a directory where a downloaded pretrained model configuration is cached if the standard cache\n",
            " |              is not used.\n",
            " |      \n",
            " |          proxies (`Dict[str, str]`, *optional*):\n",
            " |              A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',\n",
            " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
            " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether to only load local model weights and configuration files or not. If set to `True`, the model\n",
            " |              won't be downloaded from the Hub.\n",
            " |          token (`str` or *bool*, *optional*):\n",
            " |              The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from\n",
            " |              `diffusers-cli login` (stored in `~/.huggingface`) is used.\n",
            " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
            " |              The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier\n",
            " |              allowed by Git.\n",
            " |          original_config_file (`str`, *optional*):\n",
            " |              The path to the original config file that was used to train the model. If not provided, the config file\n",
            " |              will be inferred from the checkpoint file.\n",
            " |          config (`str`, *optional*):\n",
            " |              Can be either:\n",
            " |                  - A string, the *repo id* (for example `CompVis/ldm-text2im-large-256`) of a pretrained pipeline\n",
            " |                    hosted on the Hub.\n",
            " |                  - A path to a *directory* (for example `./my_pipeline_directory/`) containing the pipeline\n",
            " |                    component configs in Diffusers format.\n",
            " |          disable_mmap ('bool', *optional*, defaults to 'False'):\n",
            " |              Whether to disable mmap when loading a Safetensors model. This option can perform better when the model\n",
            " |              is on a network mount or hard drive.\n",
            " |          kwargs (remaining dictionary of keyword arguments, *optional*):\n",
            " |              Can be used to overwrite load and saveable variables (the pipeline components of the specific pipeline\n",
            " |              class). The overwritten components are passed directly to the pipelines `__init__` method. See example\n",
            " |              below for more information.\n",
            " |      \n",
            " |      Examples:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from diffusers import StableDiffusionPipeline\n",
            " |      \n",
            " |      >>> # Download pipeline from huggingface.co and cache.\n",
            " |      >>> pipeline = StableDiffusionPipeline.from_single_file(\n",
            " |      ...     \"https://huggingface.co/WarriorMama777/OrangeMixs/blob/main/Models/AbyssOrangeMix/AbyssOrangeMix.safetensors\"\n",
            " |      ... )\n",
            " |      \n",
            " |      >>> # Download pipeline from local file\n",
            " |      >>> # file is downloaded under ./v1-5-pruned-emaonly.ckpt\n",
            " |      >>> pipeline = StableDiffusionPipeline.from_single_file(\"./v1-5-pruned-emaonly.ckpt\")\n",
            " |      \n",
            " |      >>> # Enable float16 and move to GPU\n",
            " |      >>> pipeline = StableDiffusionPipeline.from_single_file(\n",
            " |      ...     \"https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5/blob/main/v1-5-pruned-emaonly.ckpt\",\n",
            " |      ...     torch_dtype=torch.float16,\n",
            " |      ... )\n",
            " |      >>> pipeline.to(\"cuda\")\n",
            " |      ```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "prompt = input(\"Enter your prompt: \")\n",
        "n_prompt = input(\"Enter your negative prompt (optional): \")\n",
        "\n",
        "if not n_prompt.strip():\n",
        "    n_prompt = \"blurry, low quality\"\n",
        "\n",
        "image = pipe(\n",
        "    prompt=prompt,\n",
        "    negative_prompt=n_prompt,\n",
        "    num_inference_steps=50,\n",
        "    guidance_scale=7.5,\n",
        "    height=512,\n",
        "    width=512\n",
        ").images[0]\n",
        "\n",
        "image.save(\"output.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "2806e93ae3354dc490ebadfbff379104",
            "d36d92d1ab1945fd9c00356fd0d54786",
            "82c68382e77f4effb557c1722178ddcf",
            "438b9f30c0cf4dac8fb7e66045d23fc8",
            "ad018fb2626e425983134e268bc27f69",
            "9d982bdc05e64295822a3a09a425adb5",
            "e6c8c322ae234709a1908d83d9467274",
            "1a12b20f921e4968a61841df5fde18d6",
            "7ac2aa1b9931492787b9f1f3a28ec588",
            "7344b201edfa4371950b673b05766aca",
            "84cc02651d72429eb4d35b6b3353fbc0",
            "4356d8acd70e4a94beb99a121665accf",
            "6a612db3f81143ce99721f3c3b6b6eb4",
            "7d3e03808a574d1cb1fe93a761bd4c0a",
            "a33a5c6783e94c2d92acb04abd89c280",
            "6a55a765bf544ceb89573dbee24034b2",
            "2f34f1dcd4804d058a7184a075c1ec9d",
            "ad5465148fda4cb88018bf07cf5a5e0d",
            "b6f03520eb644554bae8c8038c3eb59a",
            "c976f05348ff424790af85fc3e53e916",
            "041582b2fad44115aa6aac1a234581d2",
            "bba24bd6295f4b4f990dadda659d50ab"
          ]
        },
        "id": "BT4bZeBpSkxY",
        "outputId": "20fa6959-24a7-4beb-c6d6-c100d2508cff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2806e93ae3354dc490ebadfbff379104"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your prompt: A cute panda\n",
            "Enter your negative prompt (optional): \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4356d8acd70e4a94beb99a121665accf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-SIMKiueC5rE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}